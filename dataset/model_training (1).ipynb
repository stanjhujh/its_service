{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bbb770-f377-4c5d-8254-77cf3335fbbb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b0402c-6f2c-4353-8672-ee91273c86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import logging\n",
    "import json\n",
    "import smote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a706c4f-5c3e-49b2-a073-83c47e5eef2a",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926a62ff-ac39-4432-9a3f-3f398a826352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Provided Parameters - Futures Configuration\n",
    "SYMBOL = \"ES\"\n",
    "K = 0.2\n",
    "\n",
    "# # Suffix for all output files (e.g., '_v3'). Change as needed.\n",
    "SUFFIX = \"_0825\"\n",
    "VERSION = \"_v10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9231c7-f0b7-4d3e-a966-77a230b9cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'symbol': 'ES', 'price_per_point': 50.0, 'minimum_resolution_points': 0.25, 'price_per_tick': 12.5, 'maximum_drawdown': 150.0, 'points_to_maximum_drawdown': 3.0, 'targer_profit': 100, 'stoploss': 150, 'description': 'E-mini S&P 500 Futures'}\n"
     ]
    }
   ],
   "source": [
    "with open('user_parameters.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "asset_data = data['assets'][SYMBOL]\n",
    "\n",
    "# Load values into individual variables\n",
    "S_PER_POINT = asset_data['price_per_point']                    \n",
    "MIN_PRICE_RES = asset_data['minimum_resolution_points']       \n",
    "PRICE_PER_TICK = asset_data['price_per_tick']                 \n",
    "MAX_PENALTY = asset_data['maximum_drawdown']                   \n",
    "MAX_DRAWDOWN = asset_data['maximum_drawdown']                  \n",
    "POINTS_TO_MAX_DRAWDOWN = asset_data['points_to_maximum_drawdown']\n",
    "\n",
    "TARGET_PROFIT = asset_data['targer_profit']\n",
    "STOP_LOSS = asset_data['stoploss']\n",
    "\n",
    "\n",
    "print(asset_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c95f80-6770-44a9-bc41-89d68ec45e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORIES = {\n",
    "    'DATA_DIR': '../data',\n",
    "    'OUTPUT_DIR': '../output',\n",
    "    'CONFIG_DIR': '../config',\n",
    "    'PREPROCESSED_FILE': 'data/ES_preprocessed_all_features_training_zeros_corrected.csv',\n",
    "    'XGB_TRAINED_MODEL_FILE': f'./output/{SYMBOL}_XGB_model{SUFFIX}{VERSION}.pkl',\n",
    "    'RF_TRAINED_MODEL_FILE': f'./output/{SYMBOL}_RF_model{SUFFIX}{VERSION}.pkl',\n",
    "    'DOCUMENTATION_FILE': f'./output/{SYMBOL}_model_documentation{SUFFIX}{VERSION}.md',\n",
    "    'SYMBOL_SCALER_FILE': 'data/scalers/ES_scaler.pkl'\n",
    "}\n",
    "\n",
    "scaler = joblib.load('./data/ES_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e0c51-9188-40a9-90bd-87e9d1439dab",
   "metadata": {},
   "source": [
    "## Dev Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "832f8d29-1238-42f1-9c24-e2910874957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b142651e-fa95-4978-a2f8-d5d44509476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low', 'High', 'Close', 'Open', 'Low_lag1', 'High_lag1', 'Close_lag1', 'Open_lag1', 'RSI', 'MACD', 'MyWAZLTTrend', 'HABodyRangeRatio', 'MACD_lag1', 'MyWAZLTTrend_lag1', 'HABodyRangeRatio_lag1', 'HAColor_1', 'HAColor_2', 'HAColor_3', 'HAColor_lag1_1', 'HAColor_lag1_2', 'HAColor_lag1_3', 'HALongWick_1', 'HALongWick_2', 'HALongWick_3', 'HALongWick_lag1_1', 'HALongWick_lag1_2', 'HALongWick_lag1_3', 'HACloseToEMALong_1', 'HACloseToEMALong_2', 'HACloseToEMALong_3', 'HACloseToEMALong_lag1_1', 'HACloseToEMALong_lag1_2', 'HACloseToEMALong_lag1_3', 'HALowToEMALong_1', 'HALowToEMALong_2', 'HALowToEMALong_3', 'HALowToEMALong_lag1_1', 'HALowToEMALong_lag1_2', 'HALowToEMALong_lag1_3', 'HAHighToEMALong_1', 'HAHighToEMALong_2', 'HAHighToEMALong_3', 'HAHighToEMALong_lag1_1', 'HAHighToEMALong_lag1_2', 'HAHighToEMALong_lag1_3', 'HACloseToEMAShort_1', 'HACloseToEMAShort_2', 'HACloseToEMAShort_3', 'HACloseToEMAShort_lag1_1', 'HACloseToEMAShort_lag1_2', 'HACloseToEMAShort_lag1_3', 'HALowToEMAShort_1', 'HALowToEMAShort_2', 'HALowToEMAShort_3', 'HALowToEMAShort_lag1_1', 'HALowToEMAShort_lag1_2', 'HALowToEMAShort_lag1_3', 'HAHighToEMAShort_1', 'HAHighToEMAShort_2', 'HAHighToEMAShort_3', 'HAHighToEMAShort_lag1_1', 'HAHighToEMAShort_lag1_2', 'HAHighToEMAShort_lag1_3', 'EMAState_1', 'EMAState_2', 'EMAState_5', 'EMAState_11', 'EMAState_16', 'EMAState_lag1_1', 'EMAState_lag1_2', 'EMAState_lag1_5', 'EMAState_lag1_11', 'EMAState_lag1_16', 'MACDState_3', 'MACDState_7', 'RSI_lag1']\n"
     ]
    }
   ],
   "source": [
    "with open('selected_features.json', 'r') as file:\n",
    "    selected_features = json.load(file)\n",
    "\n",
    "SELECTED_FEATURES = selected_features['selected_features']\n",
    "\n",
    "print(SELECTED_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53e860-3c50-42df-a2ab-e847eb348a39",
   "metadata": {},
   "source": [
    "## Data Validation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7468576a-821d-4502-9978-b430995fb813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate input files\n",
    "def validate_files():\n",
    "    file_path = Path(DIRECTORIES['PREPROCESSED_FILE'])\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError()\n",
    "        \n",
    "    print(\"Input file found.\\n\")\n",
    "\n",
    "validate_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c1388b1-f66c-4ed7-ad67-61a9cb19f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Data Validation\n",
      "Loaded 94757 rows from data/ES_preprocessed_all_features_training_zeros_corrected.csv\n",
      "\n",
      "Total column count: 333\n",
      "\n",
      "Date parsing succeeded with format: %Y-%m-%d %H:%M:%S\n",
      "\n",
      "Remaining rows after date validation: 94757\n",
      "\n",
      "Remaining rows after precision check: 94757\n",
      "\n",
      "Bars in trading window (08:35–10:25): 94757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and validate data\n",
    "def load_data():\n",
    "    dtypes = {col: 'float64' for col in pd.read_csv(DIRECTORIES['PREPROCESSED_FILE'], nrows=0).columns[1:]}\n",
    "    dtypes['Date'] = 'str'\n",
    "    df = pd.read_csv(DIRECTORIES['PREPROCESSED_FILE'], dtype=dtypes)\n",
    "    print(f\"## Data Validation\\nLoaded {len(df)} rows from {DIRECTORIES['PREPROCESSED_FILE']}\\n\")\n",
    "    \n",
    "    n_cols = len(df.columns)\n",
    "    \n",
    "    if n_cols == 333:\n",
    "        print(f\"Total column count: {n_cols}\\n\")\n",
    "    else:\n",
    "        error_msg = f\"Expected 333 columns, found {n_cols}\"\n",
    "        raise ValueError(error_msg)\n",
    "        \n",
    "    date_formats = ['%Y-%m-%d %H:%M:%S', '%m/%d/%Y %I:%M:%S %p', '%m/%d/%y %I:%M:%S %p']\n",
    "    parsed_dates = None\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            parsed_dates = pd.to_datetime(df['Date'], format=fmt, errors='coerce')\n",
    "            if parsed_dates.notna().any():\n",
    "                print(f\"Date parsing succeeded with format: {fmt}\\n\")\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if parsed_dates is None or parsed_dates.isna().all():\n",
    "        try:\n",
    "            parsed_dates = pd.to_datetime(df['Date'], format='mixed', errors='coerce')\n",
    "            print(\"Date parsing succeeded with mixed format inference.\\n\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to parse datetime: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "    \n",
    "    df['Date'] = parsed_dates\n",
    "    invalid_dates = df['Date'].isna().sum()\n",
    "    if invalid_dates > 0:\n",
    "        print(f\"Dropped {invalid_dates} rows with invalid dates.\\n\")\n",
    "        df = df.dropna(subset=['Date'])\n",
    "    \n",
    "    print(f\"Remaining rows after date validation: {len(df)}\\n\")\n",
    "    \n",
    "    seconds = df['Date'].dt.second\n",
    "    invalid_precision = (seconds % 5 != 0).sum()\n",
    "    if invalid_precision > 0:\n",
    "        print(f\"Found {invalid_precision} rows with non-5-second precision; dropping.\\n\")\n",
    "        df = df[seconds % 5 == 0]\n",
    "    \n",
    "    print(f\"Remaining rows after precision check: {len(df)}\\n\")\n",
    "    \n",
    "    df['Time'] = df['Date'].dt.time\n",
    "    df['InWindow'] = df['Time'].apply(lambda x: pd.Timestamp('08:35:00').time() <= x <= pd.Timestamp('10:25:00').time())\n",
    "    print(f\"Bars in trading window (08:35–10:25): {df['InWindow'].sum()}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# CALL TO LOAD DATA \n",
    "df = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a0be1-8b14-436c-87a7-8a29e380f96d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02e9f096-ef35-4c68-b9f4-33617d8dc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-04 22:54:45: Calculating Base Profit from descaled data...\n",
      "(94757, 335)\n",
      "Valid Profit Indices: 94325 bars with non-zero profits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Base Profit from already descaled data\n",
    "def calculate_profits(df):\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Calculating Base Profit from descaled data...\")\n",
    "    print(df.shape)\n",
    "    # Since data is already descaled, we can use it directly\n",
    "    data = df.iloc[:, 1:].values  # Exclude Date\n",
    "    \n",
    "    long_profits = np.zeros(len(df))\n",
    "    short_profits = np.zeros(len(df))\n",
    "    max_long_profits = np.zeros(len(df))\n",
    "    max_short_profits = np.zeros(len(df))\n",
    "    df['TradingDay'] = df['Date'].dt.date\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i in range(len(df) - 6):\n",
    "        if not df['InWindow'].iloc[i]:\n",
    "            continue\n",
    "        current_day = df['TradingDay'].iloc[i]\n",
    "        if i + 6 < len(df) and df['TradingDay'].iloc[i + 1:i + 7].eq(current_day).all():\n",
    "            valid_indices.append(i)\n",
    "            open_t1 = data[i + 1, df.columns.get_loc('Open') - 1]\n",
    "            \n",
    "            # Dynamic exit logic - check each bar for stop loss/target profit\n",
    "            long_exit_bar = 6  # Default to hard exit at 6th bar\n",
    "            short_exit_bar = 6  # Default to hard exit at 6th bar\n",
    "            \n",
    "            # Check for early exits due to stop loss or target profit\n",
    "            for j in range(1, 7):\n",
    "                if i + j >= len(df):\n",
    "                    break\n",
    "                    \n",
    "                close_tj = data[i + j, df.columns.get_loc('Close') - 1]\n",
    "                low_tj = data[i + j, df.columns.get_loc('Low') - 1]\n",
    "                high_tj = data[i + j, df.columns.get_loc('High') - 1]\n",
    "                \n",
    "                # Check long position drawdown (stop loss)\n",
    "                long_drawdown = ((open_t1 - low_tj) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                if long_drawdown >= STOP_LOSS and long_exit_bar == 6:\n",
    "                    long_exit_bar = j\n",
    "                \n",
    "                # Check long position profit (target profit)\n",
    "                long_profit_at_close = ((close_tj - open_t1) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                if long_profit_at_close >= TARGET_PROFIT and long_exit_bar == 6:\n",
    "                    long_exit_bar = j\n",
    "                \n",
    "                # Check short position drawdown (stop loss)\n",
    "                short_drawdown = ((high_tj - open_t1) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                if short_drawdown >= STOP_LOSS and short_exit_bar == 6:\n",
    "                    short_exit_bar = j\n",
    "                \n",
    "                # Check short position profit (target profit)\n",
    "                short_profit_at_close = ((open_t1 - close_tj) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                if short_profit_at_close >= TARGET_PROFIT and short_exit_bar == 6:\n",
    "                    short_exit_bar = j\n",
    "            \n",
    "            # Calculate final profits based on exit bars\n",
    "            close_long_exit = data[i + long_exit_bar, df.columns.get_loc('Close') - 1]\n",
    "            close_short_exit = data[i + short_exit_bar, df.columns.get_loc('Close') - 1]\n",
    "            \n",
    "            # Long profit calculation\n",
    "            long_points = (close_long_exit - open_t1) / MIN_PRICE_RES\n",
    "            long_profit = long_points * (S_PER_POINT * MIN_PRICE_RES)\n",
    "            lows_to_exit = data[i + 1:i + long_exit_bar + 1, df.columns.get_loc('Low') - 1]\n",
    "            long_drawdown = ((open_t1 - min(lows_to_exit)) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "            long_profits[i] = -STOP_LOSS if long_drawdown >= STOP_LOSS else min(long_profit, TARGET_PROFIT)\n",
    "            \n",
    "            # Short profit calculation\n",
    "            short_points = (open_t1 - close_short_exit) / MIN_PRICE_RES\n",
    "            short_profit = short_points * (S_PER_POINT * MIN_PRICE_RES)\n",
    "            highs_to_exit = data[i + 1:i + short_exit_bar + 1, df.columns.get_loc('High') - 1]\n",
    "            short_drawdown = ((max(highs_to_exit) - open_t1) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "            short_profits[i] = -STOP_LOSS if short_drawdown >= STOP_LOSS else min(short_profit, TARGET_PROFIT)\n",
    "            \n",
    "            # Maximum profits (unchanged logic)\n",
    "            lows_t1_t6 = data[i + 1:i + 7, df.columns.get_loc('Low') - 1]\n",
    "            highs_t1_t6 = data[i + 1:i + 7, df.columns.get_loc('High') - 1]\n",
    "            \n",
    "            # Maximum Long Profit\n",
    "            if long_drawdown >= STOP_LOSS:\n",
    "                drawdown_bar = np.argmin(lows_to_exit) + 1\n",
    "                max_long_profit = ((max(highs_t1_t6[:drawdown_bar]) - open_t1) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                max_long_profits[i] = max_long_profit\n",
    "            else:\n",
    "                max_long_profit = ((max(highs_t1_t6) - open_t1) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                max_long_profits[i] = max_long_profit\n",
    "            \n",
    "            # Maximum Short Profit\n",
    "            if short_drawdown >= STOP_LOSS:\n",
    "                drawdown_bar = np.argmax(highs_to_exit) + 1\n",
    "                max_short_profit = ((open_t1 - min(lows_t1_t6[:drawdown_bar])) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                max_short_profits[i] = max_short_profit\n",
    "            else:\n",
    "                max_short_profit = ((open_t1 - min(lows_t1_t6)) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                max_short_profits[i] = max_short_profit\n",
    "    \n",
    "    valid_indices = np.array(valid_indices)\n",
    "    print(f\"Valid Profit Indices: {len(valid_indices)} bars with non-zero profits\\n\")\n",
    "    \n",
    "    return long_profits, short_profits, valid_indices\n",
    "\n",
    "\"\"\"\n",
    "Profit Calculation from Descaled Data\n",
    "Computes long and short profits over a 6-bar horizon using descaled price data. \n",
    "Calculates base profits and maximum profits, applying a $150 drawdown penalty if exceeded. \n",
    "Returns arrays of long profits, short profits, and valid indices.\n",
    "\"\"\"\n",
    "\n",
    "# Calculate profits from descaled data\n",
    "long_profits, short_profits, valid_indices = calculate_profits(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86395b99-a63d-4373-91d3-94875add9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-04 22:55:02: Scaling continuous columns...\n",
      "                 Date     Open     High      Low   Close    RSI  MACD  \\\n",
      "0 2025-04-02 08:35:00  5683.75  5684.25  5683.50  5684.0  65.68  2.23   \n",
      "1 2025-04-02 08:35:05  5684.25  5684.25  5682.25  5683.5  59.69  2.12   \n",
      "2 2025-04-02 08:35:10  5683.50  5684.00  5682.50  5683.5  56.01  2.01   \n",
      "3 2025-04-02 08:35:15  5683.50  5683.75  5681.50  5682.0  44.06  1.78   \n",
      "4 2025-04-02 08:35:20  5681.75  5682.75  5681.25  5682.0  34.44  1.58   \n",
      "\n",
      "   HABodyRangeRatio  MyWAZLTTrend  Open_lag1  ...  HACloseToEMAShort_lag1_3  \\\n",
      "0              0.25         0.931       0.00  ...                       0.0   \n",
      "1              0.13         0.918    5683.75  ...                       0.0   \n",
      "2              0.05         0.907    5684.25  ...                       0.0   \n",
      "3              0.11         0.895    5683.50  ...                       0.0   \n",
      "4              0.28         0.880    5683.50  ...                       1.0   \n",
      "\n",
      "   HALowToEMAShort_1  HALowToEMAShort_2  HALowToEMAShort_3  \\\n",
      "0                0.0                0.0                1.0   \n",
      "1                0.0                0.0                1.0   \n",
      "2                0.0                0.0                1.0   \n",
      "3                0.0                0.0                1.0   \n",
      "4                0.0                0.0                1.0   \n",
      "\n",
      "   HALowToEMAShort_lag1_1  HALowToEMAShort_lag1_2  HALowToEMAShort_lag1_3  \\\n",
      "0                     0.0                     0.0                     0.0   \n",
      "1                     0.0                     0.0                     1.0   \n",
      "2                     0.0                     0.0                     1.0   \n",
      "3                     0.0                     0.0                     1.0   \n",
      "4                     0.0                     0.0                     1.0   \n",
      "\n",
      "       Time  InWindow  TradingDay  \n",
      "0  08:35:00      True  2025-04-02  \n",
      "1  08:35:05      True  2025-04-02  \n",
      "2  08:35:10      True  2025-04-02  \n",
      "3  08:35:15      True  2025-04-02  \n",
      "4  08:35:20      True  2025-04-02  \n",
      "\n",
      "[5 rows x 336 columns]\n",
      "Continuous columns scaled successfully: Open, High, Low, Close, RSI, MACD, HABodyRangeRatio, MyWAZLTTrend, Open_lag1, High_lag1, Low_lag1, Close_lag1, RSI_lag1, MACD_lag1, HABodyRangeRatio_lag1, MyWAZLTTrend_lag1\n",
      "\n",
      "2025-08-04 22:55:02: Continuous columns scaled successfully.\n",
      "## Scaling Verification\n",
      "\n",
      "Before scaling (descaled data):\n",
      "\n",
      "Open: mean=5861.9403, std=319.7337\n",
      "\n",
      "High: mean=5862.5468, std=319.3490\n",
      "\n",
      "Low: mean=5861.3275, std=320.1028\n",
      "\n",
      "Close: mean=5861.9383, std=319.7314\n",
      "\n",
      "RSI: mean=50.8509, std=21.7481\n",
      "\n",
      "MACD: mean=0.0315, std=1.4312\n",
      "\n",
      "HABodyRangeRatio: mean=0.3181, std=0.1964\n",
      "\n",
      "MyWAZLTTrend: mean=0.0288, std=0.6107\n",
      "\n",
      "After scaling:\n",
      "\n",
      "Open: mean=0.0004, std=0.9987\n",
      "\n",
      "High: mean=0.0004, std=0.9987\n",
      "\n",
      "Low: mean=0.0004, std=0.9987\n",
      "\n",
      "Close: mean=0.0004, std=0.9987\n",
      "\n",
      "RSI: mean=0.0055, std=0.9953\n",
      "\n",
      "MACD: mean=0.0076, std=1.0105\n",
      "\n",
      "HABodyRangeRatio: mean=-0.0029, std=0.9992\n",
      "\n",
      "MyWAZLTTrend: mean=0.0049, std=0.9865\n",
      "\n",
      "Note: After scaling, continuous columns should have mean≈0 and std≈1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scale continuous columns in the dataframe\n",
    "def scale_continuous_columns(df, scaler):\n",
    "    \"\"\"Scale the continuous columns in the dataframe using the provided scaler\"\"\"\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Scaling continuous columns...\")\n",
    "    \n",
    "    continuous_cols = [\n",
    "        'Open', 'High', 'Low', 'Close', 'RSI', 'MACD', 'HABodyRangeRatio', 'MyWAZLTTrend',\n",
    "        'Open_lag1', 'High_lag1', 'Low_lag1', 'Close_lag1', 'RSI_lag1', 'MACD_lag1', 'HABodyRangeRatio_lag1', 'MyWAZLTTrend_lag1'\n",
    "    ]\n",
    "    # Only keep columns that exist in df\n",
    "    continuous_cols = [col for col in continuous_cols if col in df.columns]\n",
    "    \n",
    "    if not continuous_cols or scaler is None:\n",
    "        print(\"Warning: No continuous columns to scale or scaler is None\\n\")\n",
    "        return df\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    print(df_scaled.head())\n",
    "    \n",
    "    # Scale the continuous columns\n",
    "    df_scaled[continuous_cols] = scaler.transform(df[continuous_cols])\n",
    "    \n",
    "    print(f\"Continuous columns scaled successfully: {', '.join(continuous_cols)}\\n\")\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Continuous columns scaled successfully.\")\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "\"\"\"\n",
    "Scale Continuous Columns\n",
    "Applies the provided `StandardScaler` to normalize continuous columns (e.g., Open, High, Low, Close, RSI) in the DataFrame. \n",
    "Creates a copy of the DataFrame to avoid modifying the original data. Logs the scaled columns for verification. \n",
    "Returns the scaled DataFrame for model training.\n",
    "\"\"\"\n",
    "\n",
    "# Scale the continuous columns for training\n",
    "df_scaled = scale_continuous_columns(df, scaler)\n",
    "\n",
    "\"\"\"\n",
    "Scaling Verification\n",
    "Verifies that the scaling of continuous columns (e.g., Open, High, Low, Close, RSI) was successful. \n",
    "Logs the mean and standard deviation of these columns before and after scaling. \n",
    "Ensures that post-scaling, the mean is approximately 0 and the standard deviation is approximately 1. \n",
    "Writes results to the documentation file for transparency.\n",
    "\"\"\"\n",
    "\n",
    "# Verify scaling worked correctly\n",
    "continuous_cols = ['Open', 'High', 'Low', 'Close', 'RSI', 'MACD', 'HABodyRangeRatio', 'MyWAZLTTrend']\n",
    "continuous_cols = [col for col in continuous_cols if col in df.columns]\n",
    "\n",
    "print(\"## Scaling Verification\\n\")\n",
    "print(\"Before scaling (descaled data):\\n\")\n",
    "for col in continuous_cols:\n",
    "    col_mean = df[col].mean()\n",
    "    col_std = df[col].std()\n",
    "    print(f\"{col}: mean={col_mean:.4f}, std={col_std:.4f}\\n\")\n",
    "\n",
    "print(\"After scaling:\\n\")\n",
    "for col in continuous_cols:\n",
    "    col_mean = df_scaled[col].mean()\n",
    "    col_std = df_scaled[col].std()\n",
    "    print(f\"{col}: mean={col_mean:.4f}, std={col_std:.4f}\\n\")\n",
    "\n",
    "print(\"Note: After scaling, continuous columns should have mean≈0 and std≈1\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eff8a6c3-5acf-4160-8afa-6c9c1a58ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-04 22:55:03: Splitting data by custom cutoff and trading day...\n",
      "2025-08-04 22:55:03: Data split completed:\n",
      "                     Date      Open      High       Low     Close       RSI  \\\n",
      "88152 2025-07-07 08:35:00  1.371027  1.372317  1.369823  1.370257  1.337226   \n",
      "88153 2025-07-07 08:35:05  1.371027  1.373880  1.370603  1.373380  1.490997   \n",
      "88154 2025-07-07 08:35:10  1.372589  1.373098  1.371383  1.371038  1.384822   \n",
      "88155 2025-07-07 08:35:15  1.371027  1.371535  1.369043  1.371819  0.764703   \n",
      "88156 2025-07-07 08:35:20  1.371808  1.371535  1.369043  1.371819  0.813214   \n",
      "\n",
      "           MACD  HABodyRangeRatio  MyWAZLTTrend  Open_lag1  ...  \\\n",
      "88152 -0.360638          1.380425     -1.450387 -18.309853  ...   \n",
      "88153 -0.219423          1.380425     -1.379309   1.371027  ...   \n",
      "88154 -0.148816          1.278654     -1.335692   1.371027  ...   \n",
      "88155 -0.078209          0.617138     -1.340538   1.372589  ...   \n",
      "88156 -0.021723         -0.044378     -1.326000   1.371027  ...   \n",
      "\n",
      "       HACloseToEMAShort_lag1_3  HALowToEMAShort_1  HALowToEMAShort_2  \\\n",
      "88152                       0.0                0.0                0.0   \n",
      "88153                       0.0                0.0                0.0   \n",
      "88154                       0.0                1.0                0.0   \n",
      "88155                       0.0                1.0                0.0   \n",
      "88156                       0.0                0.0                0.0   \n",
      "\n",
      "       HALowToEMAShort_3  HALowToEMAShort_lag1_1  HALowToEMAShort_lag1_2  \\\n",
      "88152                1.0                     0.0                     0.0   \n",
      "88153                1.0                     0.0                     0.0   \n",
      "88154                0.0                     0.0                     0.0   \n",
      "88155                0.0                     1.0                     0.0   \n",
      "88156                1.0                     1.0                     0.0   \n",
      "\n",
      "       HALowToEMAShort_lag1_3      Time  InWindow  TradingDay  \n",
      "88152                     0.0  08:35:00      True  2025-07-07  \n",
      "88153                     1.0  08:35:05      True  2025-07-07  \n",
      "88154                     1.0  08:35:10      True  2025-07-07  \n",
      "88155                     0.0  08:35:15      True  2025-07-07  \n",
      "88156                     0.0  08:35:20      True  2025-07-07  \n",
      "\n",
      "[5 rows x 336 columns]\n",
      "  → Train: 69899 rows\n",
      "  → Validation: 18253 rows\n",
      "  → Test: 6605 rows starting from 2025-07-07 08:35:00\n"
     ]
    }
   ],
   "source": [
    "def split_data(df):\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Splitting data by custom cutoff and trading day...\")\n",
    "\n",
    "    # Ensure TradingDay column exists\n",
    "    df['TradingDay'] = df['Date'].dt.date\n",
    "\n",
    "    # Define test set cutoff\n",
    "    test_start_date = pd.Timestamp('2025-07-07 08:30:00')\n",
    "    \n",
    "    # Split into train+val and test\n",
    "    train_val_df = df[df['Date'] < test_start_date]\n",
    "    test_df = df[df['Date'] >= test_start_date]\n",
    "    \n",
    "    # Split train+val by trading day\n",
    "    trading_days = sorted(train_val_df['TradingDay'].unique())\n",
    "    n_days = len(trading_days)\n",
    "    train_days = trading_days[:int(0.8 * n_days)]\n",
    "    val_days = trading_days[int(0.8 * n_days):]\n",
    "\n",
    "    \n",
    "    train_df = train_val_df[train_val_df['TradingDay'].isin(train_days)]\n",
    "    val_df = train_val_df[train_val_df['TradingDay'].isin(val_days)]\n",
    "    \n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Data split completed:\")\n",
    "    print(test_df.head())\n",
    "    print(f\"  → Train: {len(train_df)} rows\")\n",
    "    print(f\"  → Validation: {len(val_df)} rows\")\n",
    "    print(f\"  → Test: {len(test_df)} rows starting from {test_df['Date'].iloc[0] if not test_df.empty else 'N/A'}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\"\"\"\n",
    "Data Splitting\n",
    "Splits the scaled DataFrame into training (70%), validation (20%), and test (10%) sets based on trading days. \n",
    "Ensures temporal consistency by assigning entire trading days to each set.\n",
    "Returns the training, validation, and test DataFrames.\n",
    "\"\"\"\n",
    "\n",
    "train_df, val_df, test_df = split_data(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95b60664-930c-4216-8153-a83572e0a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTED_FEATURES length: 76\n",
      "Found 76 features for model.\n",
      "Missing features: []\n",
      "Final selected features count: 76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def select_important_features(df, n_features=76):\n",
    "    \"\"\"Select features directly from the static SELECTED_FEATURES list to ensure exact consistency.\"\"\"\n",
    "        \n",
    "    found_features = []\n",
    "    missing_features = []\n",
    "    \n",
    "    # Debug: Print the actual SELECTED_FEATURES list\n",
    "    print(f\"SELECTED_FEATURES length: {len(SELECTED_FEATURES)}\")\n",
    "    \n",
    "    # Check which features from SELECTED_FEATURES exist in the dataframe\n",
    "    for feat in SELECTED_FEATURES:\n",
    "        if feat in df.columns:\n",
    "            found_features.append(feat)\n",
    "        else:\n",
    "            missing_features.append(feat)\n",
    "    \n",
    "    print(f\"Found {len(found_features)} features for model.\")\n",
    "    print(f\"Missing features: {missing_features}\")\n",
    "    \n",
    "    \n",
    "    # Ensure we don't exceed the number of found features\n",
    "    actual_features = found_features[:min(n_features, len(found_features))]\n",
    "    \n",
    "    print(f\"Final selected features count: {len(actual_features)}\\n\")\n",
    "    \n",
    "    # Verify we have exactly the expected number of features\n",
    "    if len(actual_features) != len(SELECTED_FEATURES):\n",
    "        print(f\"WARNING: Feature count mismatch! Expected {len(SELECTED_FEATURES)}, got {len(actual_features)}\\n\")\n",
    "        if missing_features:\n",
    "            print(f\"Missing features that need to be added to dataset: {missing_features}\\n\")\n",
    "    \n",
    "    return actual_features\n",
    "\n",
    "# Feature selection (76 features including price features)\n",
    "important_features = select_important_features(df_scaled)\n",
    "\n",
    "# Validate that we got exactly 76 features\n",
    "if len(important_features) != 76:\n",
    "    error_msg = f\"ERROR: Expected 76 features, but got {len(important_features)}\"\n",
    "    raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03d46ad1-8b0c-40d6-965a-78f89106d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Validation: Successfully selected 76 features\n",
      "\n",
      "Successfully using 76 selected features (including price features)\n",
      "\n",
      "Data Shapes - Train: (69899, 76), Val: (18253, 76), Test: (6605, 76)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get column indices for the selected features\n",
    "feature_columns = [df_scaled.columns.get_loc(f) for f in important_features if f in df_scaled.columns]\n",
    "\n",
    "print(f\"Feature Validation: Successfully selected {len(important_features)} features\\n\")\n",
    "\n",
    "# Get all features first, then select important ones\n",
    "all_feature_data_train = train_df.iloc[:, 1:333].values\n",
    "all_feature_data_val = val_df.iloc[:, 1:333].values\n",
    "all_feature_data_test = test_df.iloc[:, 1:333].values\n",
    "\n",
    "# Select only important features\n",
    "feature_data_train = all_feature_data_train[:, [i-1 for i in feature_columns]]\n",
    "feature_data_val = all_feature_data_val[:, [i-1 for i in feature_columns]]\n",
    "feature_data_test = all_feature_data_test[:, [i-1 for i in feature_columns]]\n",
    "\n",
    "print(f\"Successfully using {feature_data_train.shape[1]} selected features (including price features)\\n\")\n",
    "print(f\"Data Shapes - Train: {feature_data_train.shape}, Val: {feature_data_val.shape}, Test: {feature_data_test.shape}\\n\")\n",
    "\n",
    "train_indices = train_df.index.values\n",
    "val_indices = val_df.index.values\n",
    "test_indices = test_df.index.values #7260\n",
    "\n",
    "# Filter indices to valid profits\n",
    "train_valid_mask = np.isin(train_indices, valid_indices)\n",
    "train_valid_indices = train_indices[train_valid_mask]\n",
    "feature_data_train = feature_data_train[train_valid_mask]\n",
    "train_df_filtered = train_df.iloc[train_valid_mask]\n",
    "\n",
    "val_valid_mask = np.isin(val_indices, valid_indices)\n",
    "val_valid_indices = val_indices[val_valid_mask]\n",
    "feature_data_val = feature_data_val[val_valid_mask]\n",
    "val_df_filtered = val_df.iloc[val_valid_mask]\n",
    "\n",
    "test_valid_mask = np.isin(test_indices, valid_indices)\n",
    "test_valid_indices = test_indices[test_valid_mask]\n",
    "feature_data_test = feature_data_test[test_valid_mask]\n",
    "test_df_filtered = test_df.iloc[test_valid_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "227a8ecb-ae8d-4777-bbca-285b8a486fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "def reward(action, long_profit, short_profit):\n",
    "    if action == 0:  # Long\n",
    "        if long_profit > 30:\n",
    "            return long_profit\n",
    "        elif 10 < long_profit <= 30:\n",
    "            return -MAX_PENALTY * np.exp(-K * (long_profit - 10))\n",
    "        else:\n",
    "            return -MAX_PENALTY\n",
    "    elif action == 1:  # Short\n",
    "        if short_profit > 30:\n",
    "            return short_profit\n",
    "        elif 10 < short_profit <= 30:\n",
    "            return -MAX_PENALTY * np.exp(-K * (short_profit - 10))\n",
    "        else:\n",
    "            return -MAX_PENALTY\n",
    "    else:  # No Trade\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e33f1a00-c4be-4423-9277-1abc3db04e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_profit_threshold = 30   # Increased from 30\n",
    "\n",
    "long_ratio = 0.1 # should select top 10% trades from all available long that has highest profit\n",
    "\n",
    "short_ratio = 0.1 # should select top 10% trades from all available shorts that has highest profit\n",
    "\n",
    "# Generate action labels with improved strategy\n",
    "def generate_action_labels(long_profits, short_profits, indices, total_bars, is_test_set=False):\n",
    "\n",
    "    print(f\"Total bars: {total_bars}\")\n",
    "    \n",
    "    labels = np.full(total_bars, 2)\n",
    "    \n",
    "    profit_df = pd.DataFrame({\n",
    "        'index': indices,\n",
    "        'long_profit': long_profits,\n",
    "        'short_profit': short_profits,\n",
    "        'long_reward': [reward(0, lp, sp) for lp, sp in zip(long_profits, short_profits)],\n",
    "        'short_reward': [reward(1, lp, sp) for lp, sp in zip(long_profits, short_profits)]\n",
    "    })\n",
    "    \n",
    "    # Calculate additional metrics for better selection\n",
    "    profit_df['long_profit_ratio'] = np.where(profit_df['short_profit'] != 0, \n",
    "                                            profit_df['long_profit'] / np.abs(profit_df['short_profit']), \n",
    "                                            profit_df['long_profit'])\n",
    "    profit_df['short_profit_ratio'] = np.where(profit_df['long_profit'] != 0, \n",
    "                                             profit_df['short_profit'] / np.abs(profit_df['long_profit']), \n",
    "                                             profit_df['short_profit'])\n",
    "    \n",
    "    # Enhanced selection criteria\n",
    "    \n",
    "    # Step 1: Select high-quality trades with multiple criteria\n",
    "    long_candidates = profit_df[\n",
    "        (profit_df['long_profit'] > min_profit_threshold) & \n",
    "        (profit_df['long_reward'] > 0) &\n",
    "        (profit_df['long_profit'] > profit_df['short_profit'])\n",
    "    ].copy()\n",
    "    \n",
    "    short_candidates = profit_df[\n",
    "        (profit_df['short_profit'] > min_profit_threshold) & \n",
    "        (profit_df['short_reward'] > 0) &\n",
    "        (profit_df['short_profit'] > profit_df['long_profit'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate composite scores for ranking\n",
    "    long_candidates['composite_score'] = (\n",
    "        long_candidates['long_profit'] * 0.4 +\n",
    "        long_candidates['long_reward'] * 0.3 +\n",
    "        long_candidates['long_profit_ratio'] * 0.3\n",
    "    )\n",
    "    \n",
    "    short_candidates['composite_score'] = (\n",
    "        short_candidates['short_profit'] * 0.4 +\n",
    "        short_candidates['short_reward'] * 0.3 +\n",
    "        short_candidates['short_profit_ratio'] * 0.3\n",
    "    )\n",
    "    \n",
    "    available_long = len(long_candidates)\n",
    "    available_short = len(short_candidates)\n",
    "\n",
    "    print(f\"available_long: {available_long}\")\n",
    "    print(f\"available_short: {available_short}\")\n",
    "\n",
    "    # Calculate current trade ratios\n",
    "    long_ratio_data = available_long / total_bars if total_bars > 0 else 0\n",
    "    short_ratio_data = available_short / total_bars if total_bars > 0 else 0\n",
    "    total_trade_ratio = (available_long + available_short) / total_bars if total_bars > 0 else 0\n",
    "\n",
    "    print(f\"long_ratio: {long_ratio}\")\n",
    "    print(f\"short_ratio: {short_ratio}\")\n",
    "\n",
    "    print(f\"total_trade_ratio: {total_trade_ratio}\")\n",
    "   \n",
    "    target_long_trades should select top 10% with highest profit \n",
    "    target_short_trades should select top 10% highest profit\n",
    "\n",
    "    make any chnaging required but dont ditort the structire\n",
    "    \n",
    "    \n",
    "    # Select top trades based on composite score\n",
    "    if len(long_candidates) > 0:\n",
    "        final_long_indices = long_candidates.nlargest(target_long_trades, 'composite_score').index\n",
    "        labels[final_long_indices] = 0  # Long\n",
    "    \n",
    "    if len(short_candidates) > 0:\n",
    "        final_short_indices = short_candidates.nlargest(target_short_trades, 'composite_score').index\n",
    "        labels[final_short_indices] = 1  # Short\n",
    "\n",
    "    return labels\n",
    "\n",
    "\"\"\"\n",
    "Action Label Generation\n",
    "Generates action labels (Long, Short, No Trade) for training, validation, and test sets using profit data. Applies the `generate_action_labels` function to assign labels based on a reward function and profit thresholds. Uses filtered indices to ensure valid profit calculations. Logs the process and distribution of labels for each set.\n",
    "\"\"\"\n",
    "\n",
    "# Generate action labels\n",
    "print(f\"Generating action labels...\")\n",
    "train_labels = generate_action_labels(long_profits[train_valid_indices], short_profits[train_valid_indices], train_valid_indices, len(train_df_filtered), is_test_set=False)\n",
    "# val_labels = generate_action_labels(long_profits[val_valid_indices], short_profits[val_valid_indices], val_valid_indices, len(val_df_filtered), is_test_set=False)\n",
    "# test_labels = generate_action_labels(long_profits[test_valid_indices], short_profits[test_valid_indices], test_valid_indices, len(test_df_filtered), is_test_set=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65034dda-0905-46cc-acd9-d9a5f11a7b61",
   "metadata": {},
   "source": [
    "## Balancing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dda35bc2-528d-498c-ac69-c5bd444fd6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# SMOTE Application for Class Balancing\n",
    "# Applies SMOTE (Synthetic Minority Oversampling Technique) to balance the training dataset across three classes: Long (0), Short (1), and No Trade (2). SMOTE generates synthetic samples for minority classes to address imbalance, using 3 nearest neighbors (`k_neighbors=3`). Logs the number of samples before and after SMOTE, along with the resulting class distribution. If SMOTE fails, reverts to original data and logs the failure.\n",
    "\n",
    "# Reason for Applying SMOTE: The dataset likely has an imbalanced class distribution, with \"No Trade\" being the majority class due to selective trade criteria (e.g., high-profit thresholds). SMOTE ensures better representation of Long and Short classes, improving model performance on minority classes.\n",
    "\n",
    "# Logic: SMOTE creates synthetic samples by interpolating between existing minority class samples, preserving data characteristics. It is applied to all three classes if any are underrepresented, aiming for equal class counts in `train_labels_balanced`. The `random_state=42` ensures reproducibility, and `k_neighbors=3` controls the interpolation range.\n",
    "\n",
    "# Reason for Multi-Class SMOTE: SMOTE works for three classes by treating each minority class independently, generating synthetic samples to balance the dataset. The code validates this by logging the resulting class counts (`Long`, `Short`, `No Trade`), ensuring they are roughly equal post-SMOTE.\n",
    "# \"\"\"\n",
    "\n",
    "# print(\"Before Smote: \",np.unique(train_labels,return_counts = True))\n",
    "# # Apply SMOTE for better class balancing\n",
    "# print(f\"Applying SMOTE for class balancing...\")\n",
    "# smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "# try:# remove smote\n",
    "#     feature_data_train_balanced, train_labels_balanced = smote.fit_resample(feature_data_train, train_labels)\n",
    "    \n",
    "#     print(f\"SMOTE Applied: {len(feature_data_train)} -> {len(feature_data_train_balanced)} samples\\n\")\n",
    "#     print(f\"Balanced Distribution: Long={np.sum(train_labels_balanced==0)}, Short={np.sum(train_labels_balanced==1)}, No Trade={np.sum(train_labels_balanced==2)}\\n\")\n",
    "# except:\n",
    "#     feature_data_train_balanced = feature_data_train\n",
    "#     train_labels_balanced = train_labels\n",
    "#     print(\"SMOTE failed, using original data\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51bd8e6f-daa2-48e6-963d-cb8b0f4dac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Smote:  (array([0, 1, 2]), array([ 1260,  1223, 67098]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Smote: \",np.unique(train_labels,return_counts = True))\n",
    "feature_data_train_balanced = feature_data_train\n",
    "train_labels_balanced = train_labels\n",
    "\n",
    "# print(train_labels_balanced.countvalues())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cc148-20c6-4939-8d59-ab9e8979a578",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2172e4e-684d-491b-a539-6a162b0069ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    \"\"\"Perform hyperparameter tuning using grid search\"\"\"\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Starting hyperparameter tuning...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [1,2,4, 5, 6,7,8],\n",
    "        'learning_rate': [0.01,0.03, 0.05, 0.1,0.3,0.5],\n",
    "        'n_estimators': [150,500, 800, 1000,1500],\n",
    "        'min_child_weight': [1, 3, 5,7,9],\n",
    "        'subsample': [0.5,0.6,0.7,0.8, 0.9],\n",
    "        'colsample_bytree': [0.5,0.6,0.7,0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    class_weights = {\n",
    "        0: total_samples / (3 * class_counts[0]),  # Long\n",
    "        1: total_samples / (3 * class_counts[1]),  # Short\n",
    "        2: total_samples / (3 * class_counts[2])   # No Trade\n",
    "    }\n",
    "    sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "    \n",
    "    # Grid search with reduced combinations for speed\n",
    "    # Existing combinations preserved\n",
    "    param_combinations = [\n",
    "        {'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 800, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "        {'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 500, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.9},\n",
    "        {'max_depth': 6, 'learning_rate': 0.03, 'n_estimators': 1000, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "        {'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.8},\n",
    "        {'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 800, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.9},\n",
    "\n",
    "        # New unique combinations added below:\n",
    "        {'max_depth': 7, 'learning_rate': 0.3, 'n_estimators': 1500, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.7},\n",
    "        {'max_depth': 2, 'learning_rate': 0.01, 'n_estimators': 500, 'min_child_weight': 9, 'subsample': 0.6, 'colsample_bytree': 0.6},\n",
    "        {'max_depth': 1, 'learning_rate': 0.5, 'n_estimators': 800, 'min_child_weight': 7, 'subsample': 0.5, 'colsample_bytree': 0.5},\n",
    "        {'max_depth': 8, 'learning_rate': 0.1, 'n_estimators': 150, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.6},\n",
    "        {'max_depth': 6, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.9}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"Testing combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "        \n",
    "        # Create model with current parameters\n",
    "        model = XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_probs = model.predict_proba(X_val)\n",
    "        \n",
    "        # Custom prediction logic\n",
    "        val_predictions = np.full(len(X_val), 2)\n",
    "        for j in range(len(val_probs)):\n",
    "            long_prob, short_prob, no_trade_prob = val_probs[j]\n",
    "            if long_prob > 0.3 and long_prob > short_prob and long_prob > no_trade_prob:\n",
    "                val_predictions[j] = 0\n",
    "            elif short_prob > 0.3 and short_prob > long_prob and short_prob > no_trade_prob:\n",
    "                val_predictions[j] = 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(val_predictions == y_val)\n",
    "        \n",
    "        # Calculate balanced accuracy (giving equal weight to each class)\n",
    "        class_accuracies = []\n",
    "        for class_label in [0, 1, 2]:\n",
    "            class_mask = y_val == class_label\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_acc = np.mean(val_predictions[class_mask] == class_label)\n",
    "                class_accuracies.append(class_acc)\n",
    "        \n",
    "        balanced_accuracy = np.mean(class_accuracies) if class_accuracies else 0\n",
    "        \n",
    "        if balanced_accuracy > best_score:\n",
    "            best_score = balanced_accuracy\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71d12b81-df75-4014-850f-51efee3463bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train XGBoost model with updated parameters\"\"\"\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Training XGBoost model...\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    class_weights = {\n",
    "        0: total_samples / (3 * class_counts[0]),  # Long\n",
    "        1: total_samples / (3 * class_counts[1]),  # Short\n",
    "        2: total_samples / (3 * class_counts[2])   # No Trade\n",
    "    }\n",
    "    \n",
    "    # Updated XGBoost parameters\n",
    "    xgb_params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3,\n",
    "        'max_depth': 6,  # Increased depth\n",
    "        'learning_rate': 0.05,  # Increased learning rate\n",
    "        'n_estimators': 1000,  # More trees\n",
    "        'min_child_weight': 3,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'gamma': 0.1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1,\n",
    "        'scale_pos_weight': [class_weights[0], class_weights[1], class_weights[2]],  # Class weights\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create and train model\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c27b9452-32bb-450e-83d0-4f83e6df7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Hyperparameter Tuning\n",
    "# Performs grid search to optimize XGBoost model hyperparameters using balanced training data. \n",
    "# Tests combinations of `max_depth`, `learning_rate`, `n_estimators`, and other parameters to maximize balanced accuracy on the validation set. \n",
    "# Logs each combination's performance and the best parameters found. Returns the optimal hyperparameters for model training.\n",
    "# \"\"\"\n",
    "\n",
    "# # Hyperparameter tuning\n",
    "# best_params = hyperparameter_tuning(feature_data_train_balanced, train_labels_balanced, feature_data_val, val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "565e57ff-80e0-4405-8204-2012bf40a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': 5, 'learning_rate': 0.05, 'n_estimators': 1000, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.8}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c9cd30c2-d566-41f5-af5f-b51f925ad98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-04 23:25:17: Training ensemble models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./output/ES_RF_model_0825_v10.pkl']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble Model Training\n",
    "Trains XGBoost and Random Forest models on balanced training data for multi-class classification (Long, Short, No Trade). \n",
    "XGBoost uses optimized hyperparameters with `multi:softprob` objective, \n",
    "while Random Forest uses fixed parameters with balanced class weights. \n",
    "Saves both models to specified file paths for later use. Logs the training process and model file locations.\n",
    "\"\"\"\n",
    "\n",
    "# Train ensemble models\n",
    "print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Training ensemble models...\")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    **best_params\n",
    ")\n",
    "xgb_model.fit(\n",
    "    feature_data_train_balanced, train_labels_balanced,\n",
    "    eval_set=[(feature_data_val, val_labels)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,#problem discuss with leafs, depth\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(feature_data_train_balanced, train_labels_balanced)\n",
    "\n",
    "# Save models and feature information\n",
    "joblib.dump(xgb_model, DIRECTORIES['XGB_TRAINED_MODEL_FILE'])\n",
    "joblib.dump(rf_model, DIRECTORIES['RF_TRAINED_MODEL_FILE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "582f7b0c-167a-43ea-a89e-2d2dc77ba906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Summary:\n",
    "# Long     (0): 376\n",
    "# Short    (1): 1559\n",
    "# No Trade (2): 4670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "697982d7-78d5-4a7a-a79d-0d6b083118fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smote + thresh = 35 + long 1 + short 0.7 == >> profit $325 (long = 400, short = -75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ea4a9-9126-44f8-8d40-14ae1dac041c",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "192cc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def evaluate_model(xgb_model, rf_model, feature_data_test, test_labels, test_valid_indices, test_df, model_name):\n",
    "    \"\"\"Evaluate a model and log results with confusion matrices and detailed metrics\"\"\"\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    xgb_probs = xgb_model.predict_proba(feature_data_test)\n",
    "    rf_probs = rf_model.predict_proba(feature_data_test)\n",
    "    \n",
    "    # Ensemble averaging with weights (XGB gets higher weight due to better performance)\n",
    "    ensemble_probs = 0.7 * xgb_probs + 0.3 * rf_probs\n",
    "    \n",
    "    # Adaptive thresholding based on profit percentiles\n",
    "    predicted_actions = np.full(len(feature_data_test), 2)  # Default to No Trade\n",
    "    \n",
    "    \n",
    "    # Apply thresholds with profit considerations\n",
    "    for i in range(len(ensemble_probs)):\n",
    "        long_prob, short_prob, no_trade_prob = ensemble_probs[i]\n",
    "        \n",
    "        # Check if meets threshold and profit criteria\n",
    "        test_idx = test_valid_indices[i] if i < len(test_valid_indices) else 0\n",
    "        \n",
    "        if (long_prob > short_prob and long_prob > no_trade_prob):  \n",
    "            predicted_actions[i] = 0  # Long\n",
    "        elif (short_prob >= long_prob and short_prob > no_trade_prob):\n",
    "            predicted_actions[i] = 1  # Short\n",
    "        # Otherwise remains No Trade (2)\n",
    "    \n",
    "    # ===== SAVE DETAILED RESULTS WITH DATES AND SIGNALS =====\n",
    "    # Create detailed results DataFrame\n",
    "    results_data = []\n",
    "    signal_names = {0: 'Long', 1: 'Short', 2: 'No Trade'}\n",
    "    \n",
    "    # Get feature names if feature_data_test is a DataFrame, otherwise use generic names\n",
    "    if hasattr(feature_data_test, 'columns'):\n",
    "        feature_names = feature_data_test.columns.tolist()\n",
    "    else:\n",
    "        # If it's a numpy array, create generic feature names\n",
    "        feature_names = [f'Feature_{i}' for i in range(feature_data_test.shape[1])]\n",
    "    \n",
    "    for i in range(len(feature_data_test)):\n",
    "        test_idx = test_valid_indices[i] if i < len(test_valid_indices) else 0\n",
    "        \n",
    "        # Get date from test_df if available\n",
    "        if 'Date' in test_df.columns:\n",
    "            date = test_df.iloc[test_idx]['Date'] if test_idx < len(test_df) else None\n",
    "        elif 'datetime' in test_df.columns:\n",
    "            date = test_df.iloc[test_idx]['datetime'] if test_idx < len(test_df) else None\n",
    "        else:\n",
    "            date = None\n",
    "            \n",
    "        # Get probabilities\n",
    "        long_prob, short_prob, no_trade_prob = ensemble_probs[i]\n",
    "        \n",
    "        # Get actual and predicted signals\n",
    "        actual_signal = signal_names.get(test_labels[i], 'Unknown')\n",
    "        predicted_signal = signal_names.get(predicted_actions[i], 'Unknown')\n",
    "        \n",
    "        # Get profit information\n",
    "        long_profit = long_profits[test_idx] if test_idx < len(long_profits) else 0\n",
    "        short_profit = short_profits[test_idx] if test_idx < len(short_profits) else 0\n",
    "        \n",
    "        # Determine which profit to use based on actual signal\n",
    "        actual_profit = 0\n",
    "        if test_labels[i] == 0:  # Long\n",
    "            actual_profit = long_profit\n",
    "        elif test_labels[i] == 1:  # Short\n",
    "            actual_profit = short_profit\n",
    "            \n",
    "        # Calculate reward\n",
    "        actual_reward = reward(test_labels[i], long_profit, short_profit) if 'reward' in globals() else 0\n",
    "        \n",
    "        # Create base record with all existing fields\n",
    "        record = {\n",
    "            'Date': date,\n",
    "            'Index': test_idx,\n",
    "            'Actual_Signal': actual_signal,\n",
    "            'Predicted_Signal': predicted_signal,\n",
    "            'Long_Probability': long_prob,\n",
    "            'Short_Probability': short_prob,\n",
    "            'NoTrade_Probability': no_trade_prob,\n",
    "            'Long_Profit': long_profit,\n",
    "            'Short_Profit': short_profit,\n",
    "            'Actual_Profit': actual_profit,\n",
    "            'Actual_Reward': actual_reward,\n",
    "            'Correct_Prediction': test_labels[i] == predicted_actions[i],\n",
    "            'Model_Name': model_name\n",
    "        }\n",
    "        \n",
    "        # Add input features to the record\n",
    "        if hasattr(feature_data_test, 'iloc'):\n",
    "            # If it's a DataFrame, get features by column names\n",
    "            for feature_name in feature_names:\n",
    "                record[feature_name] = feature_data_test.iloc[i][feature_name]\n",
    "        else:\n",
    "            # If it's a numpy array, get features by index\n",
    "            for j, feature_name in enumerate(feature_names):\n",
    "                record[feature_name] = feature_data_test[i, j]\n",
    "        \n",
    "        results_data.append(record)\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = 'evaluation_results'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{output_dir}/{model_name}_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"\\n### Detailed Results Saved\\n\")\n",
    "    print(f\"Results saved to: {filename}\\n\")\n",
    "    print(f\"Total records saved: {len(results_df)}\\n\")\n",
    "    \n",
    "    # Print summary of saved data\n",
    "    print(f\"Signal Distribution in Saved Data:\\n\")\n",
    "    print(f\"Actual Signals: {results_df['Actual_Signal'].value_counts().to_dict()}\\n\")\n",
    "    print(f\"Predicted Signals: {results_df['Predicted_Signal'].value_counts().to_dict()}\\n\")\n",
    "    print(f\"Correct Predictions: {results_df['Correct_Prediction'].sum()}/{len(results_df)} ({results_df['Correct_Prediction'].mean()*100:.2f}%)\\n\")\n",
    "    \n",
    "    # ===== CONFUSION MATRIX ANALYSIS =====\n",
    "    print(f\"## {model_name} Detailed Evaluation\\n\")\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    confusion_mat = np.zeros((3, 3), dtype=int)\n",
    "    for true, pred in zip(test_labels, predicted_actions):\n",
    "        confusion_mat[true][pred] += 1\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    class_names = ['Long (0)', 'Short (1)', 'No Trade (2)']\n",
    "    print(\"### Confusion Matrix:\\n\")\n",
    "    print(\"Predicted →\\n\")\n",
    "    print(\"Actual ↓    Long    Short    No Trade\\n\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:<10} {confusion_mat[i][0]:^8} {confusion_mat[i][1]:^8} {confusion_mat[i][2]:^10}\\n\")\n",
    "    \n",
    "    # Calculate detailed statistics\n",
    "    print(\"\\n### Detailed Statistics:\\n\")\n",
    "    for true_class in range(3):\n",
    "        total_actual = np.sum(test_labels == true_class)\n",
    "        if total_actual == 0:\n",
    "            continue\n",
    "            \n",
    "        correct = confusion_mat[true_class][true_class]\n",
    "        accuracy = correct / total_actual if total_actual > 0 else 0\n",
    "        \n",
    "        print(f\"\\nActual {class_names[true_class]}:\\n\")\n",
    "        print(f\"Total Cases: {total_actual}\\n\")\n",
    "        print(f\"Correct Predictions: {correct} ({accuracy*100:.2f}%)\\n\")\n",
    "        \n",
    "        # Show misclassifications\n",
    "        for pred_class in range(3):\n",
    "            if pred_class != true_class:\n",
    "                misclassified = confusion_mat[true_class][pred_class]\n",
    "                if misclassified > 0:\n",
    "                    print(f\"Misclassified as {class_names[pred_class]}: {misclassified} ({misclassified/total_actual*100:.2f}%)\\n\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    total_correct = np.sum(np.diag(confusion_mat))\n",
    "    total_cases = np.sum(confusion_mat)\n",
    "    overall_accuracy = total_correct / total_cases if total_cases > 0 else 0\n",
    "    print(f\"\\nOverall Statistics:\\n\")\n",
    "    print(f\"Total Test Cases: {total_cases}\\n\")\n",
    "    print(f\"Total Correct Predictions: {total_correct} ({overall_accuracy*100:.2f}%)\\n\")\n",
    "    print(f\"Total Misclassifications: {total_cases - total_correct} ({(1-overall_accuracy)*100:.2f}%)\\n\")\n",
    "    \n",
    "    # ===== TRADING PERFORMANCE METRICS =====\n",
    "    # Evaluation metrics\n",
    "    actions = predicted_actions\n",
    "    long_mask = (actions == 0) & test_df['InWindow'].values\n",
    "    short_mask = (actions == 1) & test_df['InWindow'].values\n",
    "    long_trades = np.sum(long_mask)\n",
    "    short_trades = np.sum(short_mask)\n",
    "    long_profits_list = [long_profits[test_valid_indices[i]] for i in range(len(actions)) if actions[i] == 0]\n",
    "    short_profits_list = [short_profits[test_valid_indices[i]] for i in range(len(actions)) if actions[i] == 1]\n",
    "    total_long_profit = sum(long_profits_list) if long_profits_list else 0\n",
    "    total_short_profit = sum(short_profits_list) if short_profits_list else 0\n",
    "    avg_long_profit = np.mean(long_profits_list) if long_profits_list else 0\n",
    "    avg_short_profit = np.mean(short_profits_list) if short_profits_list else 0\n",
    "    long_rewards = [reward(0, long_profits[test_valid_indices[i]], short_profits[test_valid_indices[i]]) for i in range(len(actions)) if actions[i] == 0]\n",
    "    short_rewards = [reward(1, long_profits[test_valid_indices[i]], short_profits[test_valid_indices[i]]) for i in range(len(actions)) if actions[i] == 1]\n",
    "    total_long_reward = sum(long_rewards) if long_rewards else 0\n",
    "    total_short_reward = sum(short_rewards) if short_rewards else 0\n",
    "    avg_long_reward = np.mean(long_rewards) if long_rewards else 0\n",
    "    avg_short_reward = np.mean(short_rewards) if short_rewards else 0\n",
    "    \n",
    "    print(f\"### {model_name} Trading Performance\\n\")\n",
    "    print(f\"#### Long Trades\\n\")\n",
    "    print(f\"Total Base Profit: ${total_long_profit:.2f}\\n\")\n",
    "    print(f\"Average Base Profit: ${avg_long_profit:.2f}, {long_trades} trades\\n\")\n",
    "    print(f\"Total Reward: {total_long_reward:.2f}\\n\")\n",
    "    print(f\"Average Reward: {avg_long_reward:.2f}\\n\")\n",
    "    print(f\"Total Trades: {long_trades} trades, {long_trades/len(test_df)*100:.2f}% of test set bars\\n\")\n",
    "    profitable_long = sum(1 for p in long_profits_list if p > 0)\n",
    "    print(f\"Profitable Long Trades: {profitable_long}/{long_trades} ({profitable_long/max(long_trades,1)*100:.1f}%)\\n\")\n",
    "    \n",
    "    print(f\"#### Short Trades\\n\")\n",
    "    print(f\"Total Base Profit: ${total_short_profit:.2f}\\n\")\n",
    "    print(f\"Average Base Profit: ${avg_short_profit:.2f}, {short_trades} trades\\n\")\n",
    "    print(f\"Total Reward: {total_short_reward:.2f}\\n\")\n",
    "    print(f\"Average Reward: {avg_short_reward:.2f}\\n\")\n",
    "    print(f\"Total Trades: {short_trades} trades, {short_trades/len(test_df)*100:.2f}% of test set bars\\n\")\n",
    "    profitable_short = sum(1 for p in short_profits_list if p > 0)\n",
    "    print(f\"Profitable Short Trades: {profitable_short}/{short_trades} ({profitable_short/max(short_trades,1)*100:.1f}%)\\n\")\n",
    "    \n",
    "    print(f\"#### Overall Trading Statistics\\n\")\n",
    "    print(f\"Action Distribution: {np.bincount(actions, minlength=3)/len(actions)*100}\\n\")\n",
    "    print(f\"Total Profit: ${total_long_profit + total_short_profit:.2f}\\n\")\n",
    "    print(f\"Overall Accuracy: {np.mean(predicted_actions == test_labels)*100:.2f}%\\n\")\n",
    "    \n",
    "    # ===== ADDITIONAL METRICS =====\n",
    "    # Calculate precision, recall, F1-score for each class\n",
    "    print(f\"#### Precision, Recall, F1-Score by Class\\n\")\n",
    "    for class_label in range(3):\n",
    "        class_name = class_names[class_label]\n",
    "        tp = confusion_mat[class_label][class_label]  # True positives\n",
    "        fp = np.sum(confusion_mat[:, class_label]) - tp  # False positives\n",
    "        fn = np.sum(confusion_mat[class_label, :]) - tp  # False negatives\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"{class_name}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1_score:.3f}\\n\")\n",
    "    \n",
    "    #Return key metrics for comparison\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'total_profit': total_long_profit + total_short_profit,\n",
    "        'long_trades': long_trades,\n",
    "        'short_trades': short_trades,\n",
    "        'avg_long_profit': avg_long_profit,\n",
    "        'avg_short_profit': avg_short_profit,\n",
    "        'profitable_long_rate': profitable_long/max(long_trades,1)*100,\n",
    "        'profitable_short_rate': profitable_short/max(short_trades,1)*100,\n",
    "        'confusion_matrix': confusion_mat,\n",
    "        'results_file': filename,\n",
    "        'results_dataframe': results_df\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39432235-b3b8-4346-868c-7c8109900bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation (With Price Features)\n",
      "\n",
      "\n",
      "### Detailed Results Saved\n",
      "\n",
      "Results saved to: evaluation_results/Model_evaluation_results_20250804_232536.csv\n",
      "\n",
      "Total records saved: 6575\n",
      "\n",
      "Signal Distribution in Saved Data:\n",
      "\n",
      "Actual Signals: {'No Trade': 6400, 'Short': 88, 'Long': 87}\n",
      "\n",
      "Predicted Signals: {'No Trade': 6575}\n",
      "\n",
      "Correct Predictions: 6400/6575 (97.34%)\n",
      "\n",
      "## Model Detailed Evaluation\n",
      "\n",
      "### Confusion Matrix:\n",
      "\n",
      "Predicted →\n",
      "\n",
      "Actual ↓    Long    Short    No Trade\n",
      "\n",
      "Long (0)      0        0         87    \n",
      "\n",
      "Short (1)     0        0         88    \n",
      "\n",
      "No Trade (2)    0        0        6400   \n",
      "\n",
      "\n",
      "### Detailed Statistics:\n",
      "\n",
      "\n",
      "Actual Long (0):\n",
      "\n",
      "Total Cases: 87\n",
      "\n",
      "Correct Predictions: 0 (0.00%)\n",
      "\n",
      "Misclassified as No Trade (2): 87 (100.00%)\n",
      "\n",
      "\n",
      "Actual Short (1):\n",
      "\n",
      "Total Cases: 88\n",
      "\n",
      "Correct Predictions: 0 (0.00%)\n",
      "\n",
      "Misclassified as No Trade (2): 88 (100.00%)\n",
      "\n",
      "\n",
      "Actual No Trade (2):\n",
      "\n",
      "Total Cases: 6400\n",
      "\n",
      "Correct Predictions: 6400 (100.00%)\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Total Test Cases: 6575\n",
      "\n",
      "Total Correct Predictions: 6400 (97.34%)\n",
      "\n",
      "Total Misclassifications: 175 (2.66%)\n",
      "\n",
      "### Model Trading Performance\n",
      "\n",
      "#### Long Trades\n",
      "\n",
      "Total Base Profit: $0.00\n",
      "\n",
      "Average Base Profit: $0.00, 0 trades\n",
      "\n",
      "Total Reward: 0.00\n",
      "\n",
      "Average Reward: 0.00\n",
      "\n",
      "Total Trades: 0 trades, 0.00% of test set bars\n",
      "\n",
      "Profitable Long Trades: 0/0 (0.0%)\n",
      "\n",
      "#### Short Trades\n",
      "\n",
      "Total Base Profit: $0.00\n",
      "\n",
      "Average Base Profit: $0.00, 0 trades\n",
      "\n",
      "Total Reward: 0.00\n",
      "\n",
      "Average Reward: 0.00\n",
      "\n",
      "Total Trades: 0 trades, 0.00% of test set bars\n",
      "\n",
      "Profitable Short Trades: 0/0 (0.0%)\n",
      "\n",
      "#### Overall Trading Statistics\n",
      "\n",
      "Action Distribution: [  0.   0. 100.]\n",
      "\n",
      "Total Profit: $0.00\n",
      "\n",
      "Overall Accuracy: 97.34%\n",
      "\n",
      "#### Precision, Recall, F1-Score by Class\n",
      "\n",
      "Long (0): Precision=0.000, Recall=0.000, F1=0.000\n",
      "\n",
      "Short (1): Precision=0.000, Recall=0.000, F1=0.000\n",
      "\n",
      "No Trade (2): Precision=0.973, Recall=1.000, F1=0.987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Evaluation\n",
    "Evaluates the ensemble model (XGBoost and Random Forest) on the test dataset using price features. Computes predictions, confusion matrix, and trading performance metrics like total profit and trade counts. Logs detailed evaluation results, including accuracy and per-class metrics, to the documentation file. Returns a dictionary with key performance metrics for further analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Model Evaluation (With Price Features)\\n\")\n",
    "\n",
    "results = evaluate_model(xgb_model, rf_model, feature_data_test, test_labels, test_valid_indices, test_df_filtered, \"Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc074cbc-1e35-4405-9cc5-589d36547a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading data export module loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def export_trading_data(df, feature_data_test, test_labels, test_valid_indices, test_df, \n",
    "                       xgb_model, rf_model, DIRECTORIES, SYMBOL, SUFFIX, VERSION):\n",
    "    \"\"\"\n",
    "    Export trading data to CSV with the exact structure requested:\n",
    "    1. Raw OHLC data: Datetime, Open, High, Low, Close (for all signal rows including NO_TRADE)\n",
    "    2. Trade signals based on model predictions: Trade Signal, EntryDateTime, EntryPrice, ExitDateTime, ExitPrice, BaseProfit\n",
    "    \"\"\"\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Exporting trading data to CSV...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path('./output')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load asset parameters for proper profit calculation\n",
    "    with open('user_parameters.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    asset_data = data['assets'][SYMBOL]\n",
    "    S_PER_POINT = asset_data['price_per_point']\n",
    "    MIN_PRICE_RES = asset_data['minimum_resolution_points']\n",
    "    TARGET_PROFIT = asset_data['targer_profit']\n",
    "    STOP_LOSS = asset_data['stoploss']\n",
    "    \n",
    "    # ===== EXPORT 1: RAW OHLC DATA (FOR ALL SIGNAL ROWS) =====\n",
    "    print(\"Exporting raw OHLC data for all signal rows...\")\n",
    "    \n",
    "    # Get model predictions first\n",
    "    xgb_probs = xgb_model.predict_proba(feature_data_test)\n",
    "    rf_probs = rf_model.predict_proba(feature_data_test)\n",
    "    ensemble_probs = 0.7 * xgb_probs + 0.3 * rf_probs\n",
    "    \n",
    "    # Generate predictions\n",
    "    predicted_actions = np.full(len(feature_data_test), 2)  # Default to No Trade\n",
    "    \n",
    "    for i in range(len(ensemble_probs)):\n",
    "        long_prob, short_prob, no_trade_prob = ensemble_probs[i]\n",
    "        \n",
    "        if (long_prob > short_prob and long_prob > no_trade_prob):  \n",
    "            predicted_actions[i] = 0  # Long\n",
    "        elif (short_prob >= long_prob and short_prob > no_trade_prob):\n",
    "            predicted_actions[i] = 1  # Short\n",
    "    \n",
    "    # Collect indices for all signal rows (including NO_TRADE)\n",
    "    all_signal_indices = []\n",
    "    signal_names = {0: 'LONG', 1: 'SHORT', 2: 'NO_TRADE'}\n",
    "    \n",
    "    for i in range(len(feature_data_test)):\n",
    "        test_idx = test_valid_indices[i] if i < len(test_valid_indices) else 0\n",
    "        \n",
    "        # Skip if not enough future data\n",
    "        if test_idx + 6 >= len(df):\n",
    "            continue\n",
    "            \n",
    "        # Include all signals (LONG, SHORT, NO_TRADE)\n",
    "        all_signal_indices.append(test_idx)\n",
    "    \n",
    "    # Extract OHLC data for all signal rows\n",
    "    if all_signal_indices:\n",
    "        ohlc_data = df.iloc[all_signal_indices][['Date', 'Open', 'High', 'Low', 'Close']].copy()\n",
    "        ohlc_data.columns = ['Datetime', 'Open', 'High', 'Low', 'Close']\n",
    "        \n",
    "        # Save raw OHLC data\n",
    "        ohlc_filename = f'./output/{SYMBOL}_raw_ohlc_data{SUFFIX}{VERSION}.csv'\n",
    "        ohlc_data.to_csv(ohlc_filename, index=False)\n",
    "        print(f\"Raw OHLC data saved to: {ohlc_filename}\")\n",
    "        print(f\"Records: {len(ohlc_data)} (all signal rows including NO_TRADE)\")\n",
    "    else:\n",
    "        print(\"No signal rows found - no OHLC data to export\")\n",
    "        ohlc_filename = None\n",
    "    \n",
    "    # ===== EXPORT 2: TRADE SIGNALS BASED ON MODEL PREDICTIONS =====\n",
    "    print(\"Exporting trade signals based on model predictions...\")\n",
    "    \n",
    "    # Create trade signals DataFrame\n",
    "    trade_signals = []\n",
    "    \n",
    "    # Process each test prediction\n",
    "    for i in range(len(feature_data_test)):\n",
    "        test_idx = test_valid_indices[i] if i < len(test_valid_indices) else 0\n",
    "        \n",
    "        # Skip if not enough future data\n",
    "        if test_idx + 6 >= len(df):\n",
    "            continue\n",
    "            \n",
    "        # Get predicted signal\n",
    "        predicted_signal = signal_names.get(predicted_actions[i], 'NO_TRADE')\n",
    "        \n",
    "        # Entry occurs at the OPEN of the NEXT bar (not current bar)\n",
    "        entry_datetime = df.iloc[test_idx + 1]['Date']  # Next bar\n",
    "        entry_price = df.iloc[test_idx + 1]['Open']     # Open of next bar\n",
    "        \n",
    "        # Dynamic exit logic - check each bar for stop loss/target profit\n",
    "        exit_bar = 6  # Default to hard exit at 6th bar\n",
    "        exit_reason = 'HARD_EXIT'  # Track why the trade exited\n",
    "        \n",
    "        if predicted_signal in ['LONG', 'SHORT']:\n",
    "            # Check for early exits due to stop loss or target profit\n",
    "            for j in range(1, 7):\n",
    "                if test_idx + j >= len(df):\n",
    "                    break\n",
    "                    \n",
    "                close_tj = df.iloc[test_idx + j]['Close']\n",
    "                low_tj = df.iloc[test_idx + j]['Low']\n",
    "                high_tj = df.iloc[test_idx + j]['High']\n",
    "                \n",
    "                if predicted_signal == 'LONG':\n",
    "                    # Check long position drawdown (stop loss)\n",
    "                    long_drawdown = ((entry_price - low_tj) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                    if long_drawdown >= STOP_LOSS:\n",
    "                        exit_bar = j\n",
    "                        exit_reason = 'STOP_LOSS'\n",
    "                        break\n",
    "                    \n",
    "                    # Check long position profit (target profit)\n",
    "                    long_profit_at_close = ((close_tj - entry_price) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                    if long_profit_at_close >= TARGET_PROFIT:\n",
    "                        exit_bar = j\n",
    "                        exit_reason = 'TARGET_PROFIT'\n",
    "                        break\n",
    "                        \n",
    "                elif predicted_signal == 'SHORT':\n",
    "                    # Check short position drawdown (stop loss)\n",
    "                    short_drawdown = ((high_tj - entry_price) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                    if short_drawdown >= STOP_LOSS:\n",
    "                        exit_bar = j\n",
    "                        exit_reason = 'STOP_LOSS'\n",
    "                        break\n",
    "                    \n",
    "                    # Check short position profit (target profit)\n",
    "                    short_profit_at_close = ((entry_price - close_tj) / MIN_PRICE_RES) * (S_PER_POINT * MIN_PRICE_RES)\n",
    "                    if short_profit_at_close >= TARGET_PROFIT:\n",
    "                        exit_bar = j\n",
    "                        exit_reason = 'TARGET_PROFIT'\n",
    "                        break\n",
    "        \n",
    "        # Set exit datetime and price based on determined exit bar\n",
    "        exit_datetime = df.iloc[test_idx + exit_bar]['Date']\n",
    "        exit_price = df.iloc[test_idx + exit_bar]['Close']\n",
    "        \n",
    "        # Calculate base profit with proper multiplier\n",
    "        if predicted_signal == 'LONG':\n",
    "            base_profit = (exit_price - entry_price) * S_PER_POINT\n",
    "        elif predicted_signal == 'SHORT':\n",
    "            base_profit = (entry_price - exit_price) * S_PER_POINT\n",
    "        else:  # NO_TRADE\n",
    "            base_profit = 0  # No profit for NO_TRADE\n",
    "            exit_reason = 'NO_TRADE'\n",
    "            \n",
    "        trade_signals.append({\n",
    "            'Trade Signal': predicted_signal,\n",
    "            'EntryDateTime': entry_datetime,\n",
    "            'EntryPrice': entry_price,\n",
    "            'ExitDateTime': exit_datetime,\n",
    "            'ExitPrice': exit_price,\n",
    "            'BaseProfit': base_profit,\n",
    "            'ExitBar': exit_bar,\n",
    "            'ExitReason': exit_reason\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    if trade_signals:\n",
    "        trade_df = pd.DataFrame(trade_signals)\n",
    "        trade_filename = f'./output/{SYMBOL}_trade_signals{SUFFIX}{VERSION}.csv'\n",
    "        trade_df.to_csv(trade_filename, index=False)\n",
    "        print(f\"Trade signals saved to: {trade_filename}\")\n",
    "        print(f\"Trade records: {len(trade_df)}\")\n",
    "        \n",
    "        # Print signal distribution\n",
    "        signal_counts = trade_df['Trade Signal'].value_counts()\n",
    "        print(f\"Signal distribution: {dict(signal_counts)}\")\n",
    "    else:\n",
    "        print(\"No trade signals generated\")\n",
    "        trade_filename = None\n",
    "    \n",
    "    print(f\"\\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Data export completed!\")\n",
    "    \n",
    "    return {\n",
    "        'ohlc_filename': ohlc_filename,\n",
    "        'trade_filename': trade_filename,\n",
    "        'total_ohlc_records': len(all_signal_indices) if all_signal_indices else 0,\n",
    "        'total_trades': len(trade_signals) if trade_signals else 0\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Trading data export module loaded successfully!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b97e8423-58ba-4764-b14e-9b0bbb395872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data export...\n",
      "2025-08-04 22:04:43: Exporting trading data to CSV...\n",
      "Exporting raw OHLC data for all signal rows...\n",
      "Raw OHLC data saved to: ./output/ES_raw_ohlc_data_0825_v10.csv\n",
      "Records: 6575 (all signal rows including NO_TRADE)\n",
      "Exporting trade signals based on model predictions...\n",
      "Trade signals saved to: ./output/ES_trade_signals_0825_v10.csv\n",
      "Trade records: 6575\n",
      "Signal distribution: {'NO_TRADE': np.int64(6277), 'LONG': np.int64(213), 'SHORT': np.int64(85)}\n",
      "\n",
      "2025-08-04 22:04:49: Data export completed!\n",
      "All exports completed successfully!\n",
      "Files created:\n",
      "  - Raw OHLC data: ./output/ES_raw_ohlc_data_0825_v10.csv\n",
      "  - Trade signals: ./output/ES_trade_signals_0825_v10.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting data export...\")\n",
    "\n",
    "# Export trading data\n",
    "export_results = export_trading_data(df, feature_data_test, test_labels, test_valid_indices, test_df_filtered, \n",
    "                                   xgb_model, rf_model, DIRECTORIES, SYMBOL, SUFFIX, VERSION)\n",
    "\n",
    "print(f\"All exports completed successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(f\"  - Raw OHLC data: {export_results['ohlc_filename']}\")\n",
    "if export_results['trade_filename']:\n",
    "    print(f\"  - Trade signals: {export_results['trade_filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "66bc9b23-c8fa-472b-87f2-75e2c6072379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hammad/Downloads/ahsan_files_v1'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac2356-4775-47c1-8659-ae591446c393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "its"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
