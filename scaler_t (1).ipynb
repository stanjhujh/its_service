{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f1e9f0-f357-4b28-8d7f-fd31764872d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/                 preprocessor.ipynb      testing_live.ipynb\r\n",
      "model_training.ipynb  scaler_t.ipynb          user_parameters.json\r\n",
      "\u001b[01;34moutput\u001b[0m/               selected_features.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcda4337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl==3.1.0 in /home/ahsan/anaconda3/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/ahsan/anaconda3/lib/python3.10/site-packages (from openpyxl==3.1.0) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d2a926-d1f7-4584-8742-804132d832ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Duplicate rows removed. File saved at:\n",
      "data/concat_ESData_no_duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === INPUT YOUR FILE PATH HERE ===\n",
    "file_path = 'data/concat_ESData.xlsx'  # Replace with your actual file path\n",
    "\n",
    "# Convert to Path object\n",
    "file_path = Path(file_path)\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Create output file path in the same directory\n",
    "output_filename = file_path.stem + '_no_duplicates.csv'\n",
    "output_path = file_path.parent / output_filename\n",
    "\n",
    "# Save to CSV at the same location\n",
    "df_no_duplicates.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Duplicate rows removed. File saved at:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1889b935-9cc2-47a4-9c28-2748ef66df86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104179, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35eaa3e6-2e06-49a6-9928-93763dcd2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scaler data saved at:\n",
      "data/concat_ESData_no_duplicates_for_scaler.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === INPUT your _no_duplicates file path ===\n",
    "file_path = Path('data/concat_ESData_no_duplicates.csv')  # Replace with actual path\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to keep (plus 'Date' if needed for reference)\n",
    "continuous_cols = [\n",
    "    'Open', 'High', 'Low', 'Close', \n",
    "    'RSI', 'MACD', 'HABodyRangeRatio', 'MyWAZLTTrend'\n",
    "]\n",
    "\n",
    "# If you want to keep 'Date' as the first column, uncomment this:\n",
    "# if 'Date' in df.columns:\n",
    "#     continuous_cols = ['Date'] + continuous_cols\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_filtered = df[continuous_cols]\n",
    "\n",
    "# Save to new CSV with '_for_scaler' in the name\n",
    "output_filename = file_path.stem + '_for_scaler.csv'\n",
    "output_path = file_path.parent / output_filename\n",
    "\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Scaler data saved at:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2b50ba-76d2-4f94-bf8d-d4ae12e6cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Min and Max values for each column:\n",
      "\n",
      "Open: min = 4919.0, max = 6328.0\n",
      "High: min = 4922.75, max = 6328.25\n",
      "Low: min = 4918.75, max = 6327.75\n",
      "Close: min = 4919.0, max = 6328.25\n",
      "RSI: min = 0.24, max = 99.7\n",
      "MACD: min = -24.83, max = 32.32\n",
      "HABodyRangeRatio: min = 0.0, max = 1.0\n",
      "MyWAZLTTrend: min = -1.0, max = 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Path to your _for_scaler.csv file ===\n",
    "file_path = Path('data/concat_ESData_no_duplicates_for_scaler.csv')  # Replace with actual file\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print min and max for each column\n",
    "print(\"📊 Min and Max values for each column:\\n\")\n",
    "for col in df.columns:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    print(f\"{col}: min = {min_val}, max = {max_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7753cf35-ad9d-4303-8cb7-f94d78378dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New CSV with top/bottom rows saved at:\n",
      "data/concat_ESData_no_duplicates_for_scaler_updated_values.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Path to your _for_scaler.csv file ===\n",
    "file_path = Path('data/concat_ESData_no_duplicates_for_scaler.csv')  # Replace with actual path\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate min and max\n",
    "min_vals = df.min()\n",
    "max_vals = df.max()\n",
    "\n",
    "# Create top (first) row with 20% less of min values and RSI = 0\n",
    "top_row = {\n",
    "    'Open': min_vals['Open'] * 0.8,\n",
    "    'High': min_vals['High'] * 0.8,\n",
    "    'Low': min_vals['Low'] * 0.8,\n",
    "    'Close': min_vals['Close'] * 0.8,\n",
    "    'RSI': 0,\n",
    "    'MACD': min_vals['MACD'],\n",
    "    'HABodyRangeRatio': min_vals['HABodyRangeRatio'],\n",
    "    'MyWAZLTTrend': min_vals['MyWAZLTTrend']\n",
    "}\n",
    "\n",
    "# Create bottom (last) row with 20% more of max values and RSI = 100\n",
    "bottom_row = {\n",
    "    'Open': max_vals['Open'] * 1.2,\n",
    "    'High': max_vals['High'] * 1.2,\n",
    "    'Low': max_vals['Low'] * 1.2,\n",
    "    'Close': max_vals['Close'] * 1.2,\n",
    "    'RSI': 100,\n",
    "    'MACD': max_vals['MACD'],\n",
    "    'HABodyRangeRatio': max_vals['HABodyRangeRatio'],\n",
    "    'MyWAZLTTrend': max_vals['MyWAZLTTrend']\n",
    "}\n",
    "\n",
    "# Insert top row at the beginning\n",
    "df = pd.concat([pd.DataFrame([top_row]), df], ignore_index=True)\n",
    "\n",
    "# Append bottom row at the end\n",
    "df = pd.concat([df, pd.DataFrame([bottom_row])], ignore_index=True)\n",
    "\n",
    "# Save new CSV\n",
    "output_path = file_path.parent / (file_path.stem + '_updated_values.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ New CSV with top/bottom rows saved at:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bbdeb65-96b8-4e68-b248-076a1a2cad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Min and Max values for each column:\n",
      "\n",
      "Open: min = 3935.2, max = 7593.599999999999\n",
      "High: min = 3938.2, max = 7593.9\n",
      "Low: min = 3935.0, max = 7593.299999999999\n",
      "Close: min = 3935.2, max = 7593.9\n",
      "RSI: min = 0.0, max = 100.0\n",
      "MACD: min = -24.83, max = 32.32\n",
      "HABodyRangeRatio: min = 0.0, max = 1.0\n",
      "MyWAZLTTrend: min = -1.0, max = 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path('data/concat_ESData_no_duplicates_for_scaler_updated_values.csv')  \n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "print(\"📊 Min and Max values for each column:\\n\")\n",
    "for col in df.columns:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    print(f\"{col}: min = {min_val}, max = {max_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b14255-4e05-4c35-80d9-123b43f43301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-02 10:27:25: Creating scaler for continuous columns...\n",
      "✅ Scaler saved to: data/ES_scaler.pkl\n",
      "📊 Column statistics:\n",
      "Open: mean=5861.8215, std=320.1473\n",
      "High: mean=5862.4344, std=319.7642\n",
      "Low: mean=5861.2024, std=320.5156\n",
      "Close: mean=5861.8185, std=320.1470\n",
      "RSI: mean=50.7307, std=21.8508\n",
      "MACD: mean=0.0208, std=1.4163\n",
      "HABodyRangeRatio: mean=0.3187, std=0.1965\n",
      "MyWAZLTTrend: mean=0.0258, std=0.6190\n",
      "Open_lag1: mean=5861.8215, std=320.1473\n",
      "High_lag1: mean=5862.4344, std=319.7642\n",
      "Low_lag1: mean=5861.2024, std=320.5156\n",
      "Close_lag1: mean=5861.8185, std=320.1470\n",
      "RSI_lag1: mean=50.7307, std=21.8508\n",
      "MACD_lag1: mean=0.0208, std=1.4163\n",
      "HABodyRangeRatio_lag1: mean=0.3187, std=0.1965\n",
      "MyWAZLTTrend_lag1: mean=0.0258, std=0.6190\n",
      "2025-08-02 10:27:25: Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === File path to your CSV ===\n",
    "file_path = Path('data/concat_ESData_no_duplicates_for_scaler_updated_values.csv')\n",
    "\n",
    "# === Directory where scaler will be saved ===\n",
    "scaler_output_path = Path('data/ES_scaler.pkl')  # Adjust as needed\n",
    "\n",
    "# === Required base columns ===\n",
    "base_cols = [\n",
    "    'Open', 'High', 'Low', 'Close', \n",
    "    'RSI', 'MACD', 'HABodyRangeRatio', 'MyWAZLTTrend'\n",
    "]\n",
    "\n",
    "# === Load the CSV ===\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"❌ Failed to read CSV: {file_path}\\nError: {e}\")\n",
    "\n",
    "# === Check all base columns exist ===\n",
    "missing = [col for col in base_cols if col not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"❌ Missing required columns in CSV: {missing}\")\n",
    "\n",
    "# === Create *_lag1 columns by copying base values ===\n",
    "for col in base_cols:\n",
    "    df[col + '_lag1'] = df[col]\n",
    "\n",
    "# === Final list of continuous columns ===\n",
    "continuous_cols = base_cols + [col + '_lag1' for col in base_cols]\n",
    "\n",
    "# === Create and save StandardScaler ===\n",
    "def create_and_save_scaler(df, columns, output_path):\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Creating scaler for continuous columns...\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[columns])\n",
    "\n",
    "    joblib.dump(scaler, output_path)\n",
    "\n",
    "    print(\"✅ Scaler saved to:\", output_path)\n",
    "    print(\"📊 Column statistics:\")\n",
    "    for col in columns:\n",
    "        print(f\"{col}: mean={df[col].mean():.4f}, std={df[col].std():.4f}\")\n",
    "\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Done.\")\n",
    "\n",
    "# === Run the scaler function ===\n",
    "create_and_save_scaler(df, continuous_cols, scaler_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae079af-9fdc-4078-8377-c24c95df434d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
